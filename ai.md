AI in the Real World — Judge the Bot
Case 1: The Biased Hiring Bot
What’s happening:
A company uses an AI system to scan job applications and shortlist candidates. The AI has learned from past hiring data — but it ends up rejecting more female applicants with career gaps (e.g., maternity leave).
What’s problematic:
•	Bias & discrimination: Past hiring patterns favor men, and the AI repeats this bias.
•	Lack of transparency: Applicants don’t know why they were rejected.
•	Missed opportunities: Skilled candidates are unfairly filtered out.
One improvement idea:
Audit and retrain the AI on balanced, bias-checked datasets — and ensure a human reviews applications with career gaps before rejection.
________________________________________
Case 2: The Overzealous School Proctor
What’s happening:
An AI monitors students via webcam during online exams. It flags “suspicious” behavior like frequent eye movement, looking away, or fidgeting.
What’s problematic:
•	Unfair treatment: Neurodivergent students or those with certain health conditions get flagged more often.
•	Privacy risks: Continuous video/audio monitoring can feel invasive.
•	High stress: Students focus on not “looking guilty” instead of doing their best work.
One improvement idea:
Introduce a human review process for AI flags and allow alternative monitoring options for students with medical or neurodiverse needs.
________________________________________
 Detective’s Closing Note:
AI can be a powerful partner, but when left unchecked, it can quietly become an unfair judge. A responsible AI inspector knows that fairness, transparency, and a dash of human empathy are the keys to justice.

